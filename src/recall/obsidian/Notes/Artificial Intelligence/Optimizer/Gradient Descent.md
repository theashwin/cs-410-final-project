**Gradient Descent** optimization algorithms, are often used as a black-box solution to train a neural network by helping the parameters _converge_ to a _global minima_ efficiently. However, optimizer's hyperparameters can considerably affect the eventual result of training and knowing the specifics along with advantages and downfalls of optimizers can help one decide when to use a particular optimizer and what effect can fine tuning these hyperparameters can have on the model. Let's discuss the types of gradient descent first.